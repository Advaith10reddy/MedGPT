{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1110399316677344,
  "eval_steps": 500,
  "global_step": 1300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008541533205210335,
      "grad_norm": 0.6474127769470215,
      "learning_rate": 1.9943019943019945e-05,
      "loss": 1.6327,
      "step": 10
    },
    {
      "epoch": 0.01708306641042067,
      "grad_norm": 0.49449750781059265,
      "learning_rate": 1.9886039886039888e-05,
      "loss": 1.6258,
      "step": 20
    },
    {
      "epoch": 0.025624599615631006,
      "grad_norm": 0.5143863558769226,
      "learning_rate": 1.982905982905983e-05,
      "loss": 1.5977,
      "step": 30
    },
    {
      "epoch": 0.03416613282084134,
      "grad_norm": 2.456982135772705,
      "learning_rate": 1.9772079772079773e-05,
      "loss": 1.5453,
      "step": 40
    },
    {
      "epoch": 0.04270766602605167,
      "grad_norm": 0.6109153628349304,
      "learning_rate": 1.9715099715099716e-05,
      "loss": 1.4757,
      "step": 50
    },
    {
      "epoch": 0.05124919923126201,
      "grad_norm": 1.4974473714828491,
      "learning_rate": 1.965811965811966e-05,
      "loss": 1.4059,
      "step": 60
    },
    {
      "epoch": 0.059790732436472344,
      "grad_norm": 1.5555866956710815,
      "learning_rate": 1.9601139601139602e-05,
      "loss": 1.3414,
      "step": 70
    },
    {
      "epoch": 0.06833226564168268,
      "grad_norm": 1.3228235244750977,
      "learning_rate": 1.9544159544159545e-05,
      "loss": 1.2809,
      "step": 80
    },
    {
      "epoch": 0.07687379884689302,
      "grad_norm": 0.691697895526886,
      "learning_rate": 1.9487179487179488e-05,
      "loss": 1.3302,
      "step": 90
    },
    {
      "epoch": 0.08541533205210335,
      "grad_norm": 0.5441344380378723,
      "learning_rate": 1.943019943019943e-05,
      "loss": 1.2037,
      "step": 100
    },
    {
      "epoch": 0.09395686525731368,
      "grad_norm": 0.5121247172355652,
      "learning_rate": 1.9373219373219374e-05,
      "loss": 1.1722,
      "step": 110
    },
    {
      "epoch": 0.10249839846252402,
      "grad_norm": 0.4360852539539337,
      "learning_rate": 1.9316239316239317e-05,
      "loss": 1.124,
      "step": 120
    },
    {
      "epoch": 0.11103993166773436,
      "grad_norm": 0.4545866549015045,
      "learning_rate": 1.925925925925926e-05,
      "loss": 1.0875,
      "step": 130
    },
    {
      "epoch": 0.11958146487294469,
      "grad_norm": 0.5766735672950745,
      "learning_rate": 1.9202279202279203e-05,
      "loss": 1.066,
      "step": 140
    },
    {
      "epoch": 0.12812299807815503,
      "grad_norm": 1.1375508308410645,
      "learning_rate": 1.914529914529915e-05,
      "loss": 1.0287,
      "step": 150
    },
    {
      "epoch": 0.13666453128336536,
      "grad_norm": 0.39897313714027405,
      "learning_rate": 1.908831908831909e-05,
      "loss": 1.029,
      "step": 160
    },
    {
      "epoch": 0.1452060644885757,
      "grad_norm": 0.4600658118724823,
      "learning_rate": 1.9031339031339032e-05,
      "loss": 0.9827,
      "step": 170
    },
    {
      "epoch": 0.15374759769378604,
      "grad_norm": 0.453396737575531,
      "learning_rate": 1.8974358974358975e-05,
      "loss": 1.0154,
      "step": 180
    },
    {
      "epoch": 0.16228913089899638,
      "grad_norm": 0.4774487018585205,
      "learning_rate": 1.8917378917378918e-05,
      "loss": 0.963,
      "step": 190
    },
    {
      "epoch": 0.1708306641042067,
      "grad_norm": 0.6858569979667664,
      "learning_rate": 1.8860398860398864e-05,
      "loss": 0.9669,
      "step": 200
    },
    {
      "epoch": 0.17937219730941703,
      "grad_norm": 0.48864343762397766,
      "learning_rate": 1.8803418803418804e-05,
      "loss": 0.9346,
      "step": 210
    },
    {
      "epoch": 0.18791373051462737,
      "grad_norm": 0.515137255191803,
      "learning_rate": 1.874643874643875e-05,
      "loss": 0.959,
      "step": 220
    },
    {
      "epoch": 0.1964552637198377,
      "grad_norm": 0.4800487160682678,
      "learning_rate": 1.868945868945869e-05,
      "loss": 0.9427,
      "step": 230
    },
    {
      "epoch": 0.20499679692504805,
      "grad_norm": 0.6417154669761658,
      "learning_rate": 1.8632478632478636e-05,
      "loss": 0.9718,
      "step": 240
    },
    {
      "epoch": 0.21353833013025839,
      "grad_norm": 0.5327627062797546,
      "learning_rate": 1.8575498575498575e-05,
      "loss": 0.9639,
      "step": 250
    },
    {
      "epoch": 0.22207986333546872,
      "grad_norm": 2.837367057800293,
      "learning_rate": 1.851851851851852e-05,
      "loss": 0.9678,
      "step": 260
    },
    {
      "epoch": 0.23062139654067906,
      "grad_norm": 0.4924553334712982,
      "learning_rate": 1.8461538461538465e-05,
      "loss": 0.9585,
      "step": 270
    },
    {
      "epoch": 0.23916292974588937,
      "grad_norm": 0.5415403842926025,
      "learning_rate": 1.8404558404558404e-05,
      "loss": 0.8867,
      "step": 280
    },
    {
      "epoch": 0.2477044629510997,
      "grad_norm": 0.6976322531700134,
      "learning_rate": 1.834757834757835e-05,
      "loss": 0.9295,
      "step": 290
    },
    {
      "epoch": 0.25624599615631005,
      "grad_norm": 0.6483187675476074,
      "learning_rate": 1.829059829059829e-05,
      "loss": 0.9042,
      "step": 300
    },
    {
      "epoch": 0.2647875293615204,
      "grad_norm": 0.5938122272491455,
      "learning_rate": 1.8233618233618236e-05,
      "loss": 0.9034,
      "step": 310
    },
    {
      "epoch": 0.27332906256673073,
      "grad_norm": 0.5156954526901245,
      "learning_rate": 1.8176638176638176e-05,
      "loss": 0.8991,
      "step": 320
    },
    {
      "epoch": 0.28187059577194107,
      "grad_norm": 0.497390478849411,
      "learning_rate": 1.8119658119658122e-05,
      "loss": 0.9285,
      "step": 330
    },
    {
      "epoch": 0.2904121289771514,
      "grad_norm": 0.994305431842804,
      "learning_rate": 1.8062678062678065e-05,
      "loss": 0.9604,
      "step": 340
    },
    {
      "epoch": 0.29895366218236175,
      "grad_norm": 0.5294318795204163,
      "learning_rate": 1.8005698005698008e-05,
      "loss": 0.9022,
      "step": 350
    },
    {
      "epoch": 0.3074951953875721,
      "grad_norm": 0.8613611459732056,
      "learning_rate": 1.794871794871795e-05,
      "loss": 0.9147,
      "step": 360
    },
    {
      "epoch": 0.3160367285927824,
      "grad_norm": 0.5105928182601929,
      "learning_rate": 1.7891737891737894e-05,
      "loss": 0.8865,
      "step": 370
    },
    {
      "epoch": 0.32457826179799276,
      "grad_norm": 0.44716769456863403,
      "learning_rate": 1.7834757834757837e-05,
      "loss": 0.9074,
      "step": 380
    },
    {
      "epoch": 0.3331197950032031,
      "grad_norm": 0.6016931533813477,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.8937,
      "step": 390
    },
    {
      "epoch": 0.3416613282084134,
      "grad_norm": 0.5110470056533813,
      "learning_rate": 1.7720797720797723e-05,
      "loss": 0.8973,
      "step": 400
    },
    {
      "epoch": 0.3502028614136237,
      "grad_norm": 0.49109694361686707,
      "learning_rate": 1.7663817663817666e-05,
      "loss": 0.8805,
      "step": 410
    },
    {
      "epoch": 0.35874439461883406,
      "grad_norm": 0.6715020537376404,
      "learning_rate": 1.760683760683761e-05,
      "loss": 0.8513,
      "step": 420
    },
    {
      "epoch": 0.3672859278240444,
      "grad_norm": 0.5779016017913818,
      "learning_rate": 1.7549857549857552e-05,
      "loss": 0.9009,
      "step": 430
    },
    {
      "epoch": 0.37582746102925474,
      "grad_norm": 0.8509753942489624,
      "learning_rate": 1.7492877492877495e-05,
      "loss": 0.8689,
      "step": 440
    },
    {
      "epoch": 0.3843689942344651,
      "grad_norm": 0.660973072052002,
      "learning_rate": 1.7435897435897438e-05,
      "loss": 0.8854,
      "step": 450
    },
    {
      "epoch": 0.3929105274396754,
      "grad_norm": 0.7247380614280701,
      "learning_rate": 1.737891737891738e-05,
      "loss": 0.9086,
      "step": 460
    },
    {
      "epoch": 0.40145206064488576,
      "grad_norm": 0.5571202635765076,
      "learning_rate": 1.7321937321937324e-05,
      "loss": 0.8836,
      "step": 470
    },
    {
      "epoch": 0.4099935938500961,
      "grad_norm": 1.3272435665130615,
      "learning_rate": 1.7264957264957267e-05,
      "loss": 0.8836,
      "step": 480
    },
    {
      "epoch": 0.41853512705530643,
      "grad_norm": 0.8207355737686157,
      "learning_rate": 1.720797720797721e-05,
      "loss": 0.8453,
      "step": 490
    },
    {
      "epoch": 0.42707666026051677,
      "grad_norm": 0.5577322244644165,
      "learning_rate": 1.7150997150997152e-05,
      "loss": 0.8104,
      "step": 500
    },
    {
      "epoch": 0.4356181934657271,
      "grad_norm": 0.6681755781173706,
      "learning_rate": 1.7094017094017095e-05,
      "loss": 0.8346,
      "step": 510
    },
    {
      "epoch": 0.44415972667093745,
      "grad_norm": 0.6415700316429138,
      "learning_rate": 1.7037037037037038e-05,
      "loss": 0.807,
      "step": 520
    },
    {
      "epoch": 0.4527012598761478,
      "grad_norm": 0.6112965941429138,
      "learning_rate": 1.698005698005698e-05,
      "loss": 0.8722,
      "step": 530
    },
    {
      "epoch": 0.4612427930813581,
      "grad_norm": 0.6463524699211121,
      "learning_rate": 1.6923076923076924e-05,
      "loss": 0.8241,
      "step": 540
    },
    {
      "epoch": 0.46978432628656847,
      "grad_norm": 1.080014705657959,
      "learning_rate": 1.6866096866096867e-05,
      "loss": 0.802,
      "step": 550
    },
    {
      "epoch": 0.47832585949177875,
      "grad_norm": 0.6420666575431824,
      "learning_rate": 1.680911680911681e-05,
      "loss": 0.8099,
      "step": 560
    },
    {
      "epoch": 0.4868673926969891,
      "grad_norm": 0.624177098274231,
      "learning_rate": 1.6752136752136753e-05,
      "loss": 0.8034,
      "step": 570
    },
    {
      "epoch": 0.4954089259021994,
      "grad_norm": 0.8071467876434326,
      "learning_rate": 1.6695156695156696e-05,
      "loss": 0.7893,
      "step": 580
    },
    {
      "epoch": 0.5039504591074098,
      "grad_norm": 0.7185078263282776,
      "learning_rate": 1.663817663817664e-05,
      "loss": 0.7501,
      "step": 590
    },
    {
      "epoch": 0.5124919923126201,
      "grad_norm": 0.9388148784637451,
      "learning_rate": 1.6581196581196585e-05,
      "loss": 0.7768,
      "step": 600
    },
    {
      "epoch": 0.5210335255178304,
      "grad_norm": 0.7263137698173523,
      "learning_rate": 1.6524216524216525e-05,
      "loss": 0.7506,
      "step": 610
    },
    {
      "epoch": 0.5295750587230408,
      "grad_norm": 0.5506830215454102,
      "learning_rate": 1.6467236467236468e-05,
      "loss": 0.7649,
      "step": 620
    },
    {
      "epoch": 0.5381165919282511,
      "grad_norm": 0.739236056804657,
      "learning_rate": 1.641025641025641e-05,
      "loss": 0.7236,
      "step": 630
    },
    {
      "epoch": 0.5466581251334615,
      "grad_norm": 0.946633517742157,
      "learning_rate": 1.6353276353276354e-05,
      "loss": 0.7371,
      "step": 640
    },
    {
      "epoch": 0.5551996583386718,
      "grad_norm": 0.8381460905075073,
      "learning_rate": 1.6296296296296297e-05,
      "loss": 0.7472,
      "step": 650
    },
    {
      "epoch": 0.5637411915438821,
      "grad_norm": 0.750682532787323,
      "learning_rate": 1.623931623931624e-05,
      "loss": 0.7394,
      "step": 660
    },
    {
      "epoch": 0.5722827247490925,
      "grad_norm": 0.5716701149940491,
      "learning_rate": 1.6182336182336186e-05,
      "loss": 0.7615,
      "step": 670
    },
    {
      "epoch": 0.5808242579543028,
      "grad_norm": 0.7110501527786255,
      "learning_rate": 1.6125356125356125e-05,
      "loss": 0.7596,
      "step": 680
    },
    {
      "epoch": 0.5893657911595132,
      "grad_norm": 0.592549741268158,
      "learning_rate": 1.6068376068376072e-05,
      "loss": 0.7652,
      "step": 690
    },
    {
      "epoch": 0.5979073243647235,
      "grad_norm": 0.6923067569732666,
      "learning_rate": 1.601139601139601e-05,
      "loss": 0.7174,
      "step": 700
    },
    {
      "epoch": 0.6064488575699338,
      "grad_norm": 0.5232965350151062,
      "learning_rate": 1.5954415954415958e-05,
      "loss": 0.6874,
      "step": 710
    },
    {
      "epoch": 0.6149903907751442,
      "grad_norm": 0.8096674680709839,
      "learning_rate": 1.5897435897435897e-05,
      "loss": 0.7194,
      "step": 720
    },
    {
      "epoch": 0.6235319239803545,
      "grad_norm": 0.7923455834388733,
      "learning_rate": 1.584045584045584e-05,
      "loss": 0.7422,
      "step": 730
    },
    {
      "epoch": 0.6320734571855648,
      "grad_norm": 0.6661001443862915,
      "learning_rate": 1.5783475783475787e-05,
      "loss": 0.7336,
      "step": 740
    },
    {
      "epoch": 0.6406149903907752,
      "grad_norm": 0.6724006533622742,
      "learning_rate": 1.5726495726495726e-05,
      "loss": 0.7154,
      "step": 750
    },
    {
      "epoch": 0.6491565235959855,
      "grad_norm": 0.7741084098815918,
      "learning_rate": 1.5669515669515672e-05,
      "loss": 0.7129,
      "step": 760
    },
    {
      "epoch": 0.6576980568011959,
      "grad_norm": 0.6651636958122253,
      "learning_rate": 1.5612535612535612e-05,
      "loss": 0.6704,
      "step": 770
    },
    {
      "epoch": 0.6662395900064062,
      "grad_norm": 0.6699370741844177,
      "learning_rate": 1.555555555555556e-05,
      "loss": 0.7339,
      "step": 780
    },
    {
      "epoch": 0.6747811232116165,
      "grad_norm": 0.631229043006897,
      "learning_rate": 1.5498575498575498e-05,
      "loss": 0.7408,
      "step": 790
    },
    {
      "epoch": 0.6833226564168268,
      "grad_norm": 0.5559128522872925,
      "learning_rate": 1.5441595441595444e-05,
      "loss": 0.708,
      "step": 800
    },
    {
      "epoch": 0.6918641896220371,
      "grad_norm": 0.6927840709686279,
      "learning_rate": 1.5384615384615387e-05,
      "loss": 0.7543,
      "step": 810
    },
    {
      "epoch": 0.7004057228272474,
      "grad_norm": 0.7263584136962891,
      "learning_rate": 1.532763532763533e-05,
      "loss": 0.7049,
      "step": 820
    },
    {
      "epoch": 0.7089472560324578,
      "grad_norm": 0.8349190950393677,
      "learning_rate": 1.5270655270655273e-05,
      "loss": 0.6985,
      "step": 830
    },
    {
      "epoch": 0.7174887892376681,
      "grad_norm": 0.7235918045043945,
      "learning_rate": 1.5213675213675214e-05,
      "loss": 0.6952,
      "step": 840
    },
    {
      "epoch": 0.7260303224428785,
      "grad_norm": 0.6778665781021118,
      "learning_rate": 1.5156695156695157e-05,
      "loss": 0.6964,
      "step": 850
    },
    {
      "epoch": 0.7345718556480888,
      "grad_norm": 0.6013734936714172,
      "learning_rate": 1.50997150997151e-05,
      "loss": 0.735,
      "step": 860
    },
    {
      "epoch": 0.7431133888532991,
      "grad_norm": 0.6615345478057861,
      "learning_rate": 1.5042735042735043e-05,
      "loss": 0.7147,
      "step": 870
    },
    {
      "epoch": 0.7516549220585095,
      "grad_norm": 0.7172817587852478,
      "learning_rate": 1.4985754985754988e-05,
      "loss": 0.7071,
      "step": 880
    },
    {
      "epoch": 0.7601964552637198,
      "grad_norm": 1.113205909729004,
      "learning_rate": 1.4928774928774929e-05,
      "loss": 0.7081,
      "step": 890
    },
    {
      "epoch": 0.7687379884689302,
      "grad_norm": 0.6255632042884827,
      "learning_rate": 1.4871794871794874e-05,
      "loss": 0.6951,
      "step": 900
    },
    {
      "epoch": 0.7772795216741405,
      "grad_norm": 0.8017781972885132,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.7387,
      "step": 910
    },
    {
      "epoch": 0.7858210548793508,
      "grad_norm": 0.807090699672699,
      "learning_rate": 1.475783475783476e-05,
      "loss": 0.7075,
      "step": 920
    },
    {
      "epoch": 0.7943625880845612,
      "grad_norm": 0.8854222893714905,
      "learning_rate": 1.4700854700854703e-05,
      "loss": 0.7238,
      "step": 930
    },
    {
      "epoch": 0.8029041212897715,
      "grad_norm": 0.7412013411521912,
      "learning_rate": 1.4643874643874645e-05,
      "loss": 0.6737,
      "step": 940
    },
    {
      "epoch": 0.8114456544949818,
      "grad_norm": 0.6612101793289185,
      "learning_rate": 1.4586894586894588e-05,
      "loss": 0.671,
      "step": 950
    },
    {
      "epoch": 0.8199871877001922,
      "grad_norm": 0.603352963924408,
      "learning_rate": 1.4529914529914531e-05,
      "loss": 0.6834,
      "step": 960
    },
    {
      "epoch": 0.8285287209054025,
      "grad_norm": 0.588797926902771,
      "learning_rate": 1.4472934472934474e-05,
      "loss": 0.6942,
      "step": 970
    },
    {
      "epoch": 0.8370702541106129,
      "grad_norm": 0.7458504438400269,
      "learning_rate": 1.4415954415954416e-05,
      "loss": 0.7468,
      "step": 980
    },
    {
      "epoch": 0.8456117873158232,
      "grad_norm": 0.7102400064468384,
      "learning_rate": 1.435897435897436e-05,
      "loss": 0.6536,
      "step": 990
    },
    {
      "epoch": 0.8541533205210335,
      "grad_norm": 0.6840359568595886,
      "learning_rate": 1.4301994301994305e-05,
      "loss": 0.7188,
      "step": 1000
    },
    {
      "epoch": 0.8626948537262439,
      "grad_norm": 0.7439097166061401,
      "learning_rate": 1.4245014245014246e-05,
      "loss": 0.6604,
      "step": 1010
    },
    {
      "epoch": 0.8712363869314542,
      "grad_norm": 0.6524509787559509,
      "learning_rate": 1.4188034188034189e-05,
      "loss": 0.7403,
      "step": 1020
    },
    {
      "epoch": 0.8797779201366646,
      "grad_norm": 0.7648531198501587,
      "learning_rate": 1.4131054131054132e-05,
      "loss": 0.7238,
      "step": 1030
    },
    {
      "epoch": 0.8883194533418749,
      "grad_norm": 0.6838438510894775,
      "learning_rate": 1.4074074074074075e-05,
      "loss": 0.7294,
      "step": 1040
    },
    {
      "epoch": 0.8968609865470852,
      "grad_norm": 0.713559091091156,
      "learning_rate": 1.4017094017094018e-05,
      "loss": 0.6834,
      "step": 1050
    },
    {
      "epoch": 0.9054025197522956,
      "grad_norm": 0.7668052315711975,
      "learning_rate": 1.3960113960113961e-05,
      "loss": 0.7106,
      "step": 1060
    },
    {
      "epoch": 0.9139440529575059,
      "grad_norm": 0.7432243824005127,
      "learning_rate": 1.3903133903133906e-05,
      "loss": 0.68,
      "step": 1070
    },
    {
      "epoch": 0.9224855861627163,
      "grad_norm": 0.6335190534591675,
      "learning_rate": 1.3846153846153847e-05,
      "loss": 0.7119,
      "step": 1080
    },
    {
      "epoch": 0.9310271193679266,
      "grad_norm": 0.821283757686615,
      "learning_rate": 1.3789173789173791e-05,
      "loss": 0.6732,
      "step": 1090
    },
    {
      "epoch": 0.9395686525731369,
      "grad_norm": 0.8397311568260193,
      "learning_rate": 1.3732193732193733e-05,
      "loss": 0.6985,
      "step": 1100
    },
    {
      "epoch": 0.9481101857783472,
      "grad_norm": 0.8029382228851318,
      "learning_rate": 1.3675213675213677e-05,
      "loss": 0.6997,
      "step": 1110
    },
    {
      "epoch": 0.9566517189835575,
      "grad_norm": 0.8381825685501099,
      "learning_rate": 1.3618233618233619e-05,
      "loss": 0.7398,
      "step": 1120
    },
    {
      "epoch": 0.9651932521887678,
      "grad_norm": 0.71820467710495,
      "learning_rate": 1.3561253561253562e-05,
      "loss": 0.6685,
      "step": 1130
    },
    {
      "epoch": 0.9737347853939782,
      "grad_norm": 0.8232391476631165,
      "learning_rate": 1.3504273504273506e-05,
      "loss": 0.6762,
      "step": 1140
    },
    {
      "epoch": 0.9822763185991885,
      "grad_norm": 0.912804365158081,
      "learning_rate": 1.3447293447293447e-05,
      "loss": 0.7264,
      "step": 1150
    },
    {
      "epoch": 0.9908178518043989,
      "grad_norm": 0.603588342666626,
      "learning_rate": 1.3390313390313392e-05,
      "loss": 0.7191,
      "step": 1160
    },
    {
      "epoch": 0.9993593850096092,
      "grad_norm": 0.840475857257843,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.6904,
      "step": 1170
    },
    {
      "epoch": 1.0085415332052103,
      "grad_norm": 0.7360270023345947,
      "learning_rate": 1.3276353276353278e-05,
      "loss": 0.7567,
      "step": 1180
    },
    {
      "epoch": 1.0170830664104207,
      "grad_norm": 0.8197722434997559,
      "learning_rate": 1.321937321937322e-05,
      "loss": 0.6759,
      "step": 1190
    },
    {
      "epoch": 1.025624599615631,
      "grad_norm": 0.918023407459259,
      "learning_rate": 1.3162393162393164e-05,
      "loss": 0.6482,
      "step": 1200
    },
    {
      "epoch": 1.0341661328208414,
      "grad_norm": 0.886110782623291,
      "learning_rate": 1.3105413105413107e-05,
      "loss": 0.7065,
      "step": 1210
    },
    {
      "epoch": 1.0427076660260517,
      "grad_norm": 0.9231897592544556,
      "learning_rate": 1.304843304843305e-05,
      "loss": 0.7019,
      "step": 1220
    },
    {
      "epoch": 1.051249199231262,
      "grad_norm": 0.6738861799240112,
      "learning_rate": 1.2991452991452993e-05,
      "loss": 0.6825,
      "step": 1230
    },
    {
      "epoch": 1.0597907324364724,
      "grad_norm": 0.855257511138916,
      "learning_rate": 1.2934472934472934e-05,
      "loss": 0.6952,
      "step": 1240
    },
    {
      "epoch": 1.0683322656416827,
      "grad_norm": 0.866819441318512,
      "learning_rate": 1.2877492877492879e-05,
      "loss": 0.6858,
      "step": 1250
    },
    {
      "epoch": 1.076873798846893,
      "grad_norm": 0.8166062831878662,
      "learning_rate": 1.2820512820512823e-05,
      "loss": 0.6461,
      "step": 1260
    },
    {
      "epoch": 1.0854153320521034,
      "grad_norm": 1.028923511505127,
      "learning_rate": 1.2763532763532764e-05,
      "loss": 0.6911,
      "step": 1270
    },
    {
      "epoch": 1.0939568652573137,
      "grad_norm": 0.8555133938789368,
      "learning_rate": 1.2706552706552709e-05,
      "loss": 0.6713,
      "step": 1280
    },
    {
      "epoch": 1.102498398462524,
      "grad_norm": 0.8439998030662537,
      "learning_rate": 1.264957264957265e-05,
      "loss": 0.6293,
      "step": 1290
    },
    {
      "epoch": 1.1110399316677344,
      "grad_norm": 0.6356589198112488,
      "learning_rate": 1.2592592592592593e-05,
      "loss": 0.6942,
      "step": 1300
    }
  ],
  "logging_steps": 10,
  "max_steps": 3510,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.226241523781468e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
